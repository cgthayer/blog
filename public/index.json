
[{"content":" Recent Posts # ","date":"9 January 2026","externalUrl":null,"permalink":"/","section":"Learn Tinker Share Blog","summary":"Recent Posts # ","title":"Learn Tinker Share Blog","type":"page"},{"content":"","date":"9 January 2026","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"tl;dr become prod-ready and bullet-proof using agentic evals testing.\nThose of us who write AI applications fall into two categories,: the nervous and the prod-ready. It\u0026rsquo;s usually based on whether you have run a production system, with real users, over time. When you have, you learn the first key differentiator is \u0026ldquo;evals\u0026rdquo;, but not specifically traditional ML evals. Simply put, it\u0026rsquo;s whether you have an automated way to test and score the quality of your systems.\nA year ago, I was working on several projects, and one needed a much higher degree of quality and attention because it was medical. The project was 100% about trust. Not only did it need to be highly accurate, but it had many moving parts including over a dozen agents. My fellow developer did an amazing job of making it accurate, thorough, and consistent. There were a few key tricks to make this coordination work well, but what really taught me was how important having agentic evals are for making a high quality system. In fact, your agentic evals are more valuable IP than your prompts (or code), given the speed we\u0026rsquo;re moving at.\nFundamentals # TDD: Test-Driven Development, at it\u0026rsquo;s core, has a simple principle:\nWrite a test that fails, then code until it works. We still need this safety net, but with AI systems, the results are far less binary, so we often need a score or a full rubric to know when we\u0026rsquo;ve reached our goals. Unittests that pass or fail is a start, but too brittle with AI work. Whether or not a project has agentic evals is a clear signal (like a code smell) about it\u0026rsquo;s production readiness.\nWhen working on any system you need a way to answer \u0026ldquo;Does it work?\u0026rdquo; and with AI \u0026ldquo;Does it work well?\u0026rdquo;, so the definition of done has shifted a little.\nEvals: ML work and agent system development are different enough that the existing tools don\u0026rsquo;t quite fit. ML evals are the common tool today to achieve quality testing, and they\u0026rsquo;re a natural fit for those coming out of ML research and engineering. If you\u0026rsquo;ve trained a classifer, model, or even a trivial neural net, you\u0026rsquo;ve probably used ML evals. But these are designed for a different problem where you have lots of input data. When writing a prompt, agent, or workflow of agents, you\u0026rsquo;re starting with a bit of test data and growing it over time. Current eval tools often feel heavyweight for this situation.\nSo, I\u0026rsquo;m calling out something between TDD unittests and ML evals. If they got married today, their kid would be agentic evals \u0026ndash;some of each. They\u0026rsquo;re the blend of \u0026ldquo;Make it work\u0026rdquo; and \u0026ldquo;Make it right\u0026rdquo; for AI. (thanks Kent Beck).\nWhen I say \u0026ldquo;agentic evals\u0026rdquo;, I\u0026rsquo;m simply talking about these principles:\nA way to score your output. A set of inputs to test. A fast and simple way to run this. Don\u0026rsquo;t get me wrong, if you wrote ML evals using existing tools, you rock! and you\u0026rsquo;re already ahead of the game. But if you\u0026rsquo;re just starting out, you might find it easier to start with agentic evals.\nProblems at Every Stage # Let\u0026rsquo;s go over some of my past failure. If you don\u0026rsquo;t have measures and metrics, a myriad of problems come up at every phase of development:\nProblems you hit in Dev # When your team is first writing the application, and trying to reach that crucial step of getting the minimum feature set functioning:\nPrompt Churn: You keep changing the prompt to \u0026ldquo;make it work\u0026rdquo; but with every prompt change you might improve the output for one input, but hurt other cases. This is a game of whack-a-mole where fixing one problem reverts progress on others. Error Amplification: In a prompt change, a shift early in the pipeline causes downstream agents to different inputs, and with each step errors grow. This is like a game of telephone, where the output gets worse and less predictable as the chain gets longer. Prompt Bloat: In an attempt to handle different inputs the prompt keeps growing because you\u0026rsquo;re afraid to update the start of the prompt. You may even wind up with a prompt that contradicts itself if many people on your team are contributing. Ultimately, you\u0026rsquo;ll even cause the context window to grow too large and the LLM will be unable to pay attention to all of it when trying to get something done (see context-rot below). How Agentic Evals Help:\nPrompt Churn: Agentic evals testing gives you a set of inputs that you can re-evaluate when the prompt changes to ensure your old fixes aren\u0026rsquo;t regressing. Every bug fix then becomes durable. You can see when you cause a test case to regress. Error Amplification: You can tell when your agentic pipeline gets worse, and you can dig into which prompt degraded and it\u0026rsquo;s downstream effect. Prompt Bloat: now you have a way to verify if trimming the prompt hurts your score, so you can be confident your update is safe. Problems with Beta Testing # You finally get a working system in front of real users, and start getting real feedback.\nChange Fear: You want to fix something a user has called out, or handle a new case, but you\u0026rsquo;re afraid to make a change (see prompt churn). It feels like a house of cards \u0026ndash;too fragile to touch. Cost Control: Now that you have real users you\u0026rsquo;re shocked at how much tokens are costing you. You wish you could decrease the context size of your prompts but you don\u0026rsquo;t know if that\u0026rsquo;s safe. Maybe you could use a smaller cheaper model or an open-source model, but again you don\u0026rsquo;t know if that\u0026rsquo;s safe. This is the difference between having a profitable business and being out of business. Looking at your budget, your new airplane is outta-runway, and you probably have to move forward, burning through investor money to prove your product-market-fit. Tooling: You likely have tool-calling (tool-use) and maybe MCPs but since you didn\u0026rsquo;t develop these, you discover their short comings as you go. Now you realize your WebFetchTool only grabs the first 8k of a web site, etc. This is another issue you discover but have to table until your startup has income. Timeliness: Because the development process takes time, staleness creeps in. For example, you built based on web pages that got updated, or other data that are growing. Now you realize you need to re-test with newer data but have to go through that manually. Maybe you even used a model that was trained half a year ago so you absolutely need to add a web tool to get up-to-date news, or you need to switch to the latest greatest frontier model. Neither of which feels safe. Your product is in danger of being past its sell-by-date. Focus: The map is not the territory. You had a theory about what\u0026rsquo;s valuable to your users. Putting it in front of people changes all that, plus you learn about a lot of problems you hadn\u0026rsquo;t considered. Now your prompt is trying to juggle too much, so you\u0026rsquo;re wondering if you can safely update it to handle the new use cases in one agent, or if you need to split the work up into separate prompts or agents. How Agentic Evals Help:\nChange Fear: Handling new cases, you\u0026rsquo;re sure you haven\u0026rsquo;t hurt old cases. Focus: Splitting a prompt in two becomes safe. The tests are the same, it\u0026rsquo;s just the implementation that changes. Timeliness: Testing with a new model is easy, so you can safely upgrade to one of the latest frontier models. Cost Control: You have tests to let you try out smaller cheaper models, or understand where they fall down and need better prompts. Tooling: You will naturally catch shifts in tool calls that impact your quality, as well as having a way to try different tools safely. Problems in Production # You\u0026rsquo;ve ironed out a lot of issues by this stage and you\u0026rsquo;re finally going GA with a full launch. Now it\u0026rsquo;s hard to respond to folks individually and understand how all your users are doing.\nFlying Blind: Things go well and you grow like crazy. But with alerts and logs you only investigate the most severe issues. Things feel fragile, like the next update could accidentally trigger a surprising amount of user churn. You find you\u0026rsquo;re randomly digging into your trace spans to figure out how little problems are causing error amplification. Then Context-Debt: As real user data builds up, which is great, it creates more context to manage going into your prompts. It feels like tech-debt, where you have a nagging demon on your shoulder that you don\u0026rsquo;t have time to banish. The data build up has lots of negative side effects, and it\u0026rsquo;s time for some serious context engineering:\nLatency Hit: . In turn, this slows down answer latency and TTFT (time to first token). You\u0026rsquo;re like a cooking frog, the slow down is getting gradually worse over time, so you don\u0026rsquo;t notice you\u0026rsquo;re boiling, until your CEO realizes too late that it\u0026rsquo;s caused a big customer to leave. Context Costs: The context window has a direct relationship to tokens and costs, which just keeps creeping up. Perhaps the worst part is it effects the best and most loyal customers the most. Context Rot: For the same reason (this context build-up), the context windows filling up cause the LLM to lose track of what it\u0026rsquo;s doing and emit really poor results. Now you have to scramble to implement a better context compression scheme and a real memory sub-system for your use-cases. How Agentic Evals Help:\nLatency Hit: In addition to quality scores, measuring timing is something you get easily. Now you can detect latency changes and make smart trade-offs. Context Rot: Likewise you can be very intentional about how you trim your context, and you have a basis for selecting a memory subsystem, for example. Context Costs: The same is true for tokens (and input and output length); they\u0026rsquo;re all easy to track with agentic evals. From Newbie to Prod-Ready # When building a project or managing one in production, you want a lot of safety nets. But let\u0026rsquo;s take just this one critical step first.\nWhat you need first is testing. I call this agentic evals (or Agentic Evaluations Testing), to distinguish from unittests and end-to-end tests. But agentic evals really come from ML evals (evaluations) used for testing models, which generally have much larger training and test sets. Instead, I mean something like benchmarks that produce a score, or a set of scores for both individual prompts (and agents) as well as multi-agent systems (workflows, pipelines, swarms, etc.).\nThese agentic evals are lightweight compared to ML evals, which are a big scary thing (unless you\u0026rsquo;re accustomed to ML research). If you try out some of the tools you\u0026rsquo;ll find them daunting. But there\u0026rsquo;s an easier first step, that can naturally fall out of the process of writing and trying out the initial prompts. If you already have code, that\u0026rsquo;s okay too, you may even have a CSV with a pile of inputs to try out, or just a spreadsheet of inputs and outputs.\nHow to start and what to build\nA way to score your output. A set of inputs to test, and optionally outputs A fast and simple way to run these, like a benchmark. Scoring: This could be code that checks a regex, but it often becomes a single prompt with a clear rubric, which we\u0026rsquo;ll call an LLM-judge, or it could be a panel of agents that review and score different aspects of the results, aka the LLM-jury. To start, it may help to keep thinking binary and have the judge fail when there\u0026rsquo;s some clearly bad output.\nInputs: Although you may hard code these initially, often these grow into a either a CSV file, or a directory with a test input per file. This makes it trivial to add more cases, for the whole team. Later down the line, this can be a simple database or other data store so you can build better tooling and automation.\nBenchmarking: Just a test runner, so even pytest (pytest-benchmark) can be a useful starting point. Often these are easy enough to quickly vibe code, which can make the outputs help track timing and token costs. As you level up, you\u0026rsquo;ll tie this into CI/CD so that PRs that cause any regressions are nicely called out.\nLots more to say, but I\u0026rsquo;ll stop here so you can let me know what you\u0026rsquo;d like to talk about next.\n","date":"9 January 2026","externalUrl":null,"permalink":"/posts/real-ai-engineer-differentiator/","section":"Posts","summary":"tl;dr become prod-ready and bullet-proof using agentic evals testing.\nThose of us who write AI applications fall into two categories,: the nervous and the prod-ready. It‚Äôs usually based on whether you have run a production system, with real users, over time. When you have, you learn the first key differentiator is ‚Äúevals‚Äù, but not specifically traditional ML evals. Simply put, it‚Äôs whether you have an automated way to test and score the quality of your systems.\n","title":"What Separates the Real AI SW Engineers","type":"posts"},{"content":" From GraphDB Ontologies to Embeddings: Modeling Fuzzy Relationships # Keywords: GraphDB, VectorDB, Ontology, Embedding Vectors\nI was talking to a friend who uses a graphDB in their AI app and has run into an interesting problem. The quality is low because there are often many terms for the same thing, and many terms with multiple meanings depending on context. For example, if you\u0026rsquo;re looking for people to go on a walk with, it\u0026rsquo;s likely that people who like running or hiking are good candidates, but this may not be well represented in your graphDB. There\u0026rsquo;s a traditional way to solve this and more modern one using a vectorDB (weaviate, postgres with pgvector, etc.).\nOne approach is to create an ontology around this idea of activities and encode how related they are as a number. For example, if I like running maybe that means there\u0026rsquo;s an 80% chance I\u0026rsquo;ll like walking (even though I didn\u0026rsquo;t say so), and if I like hiking perhaps there\u0026rsquo;s a 90% chance I\u0026rsquo;ll like walking. Plus, in the other direction, if I like walking I\u0026rsquo;m also 90% likely to like hiking.\nIMG-SOURCE But wait, if I like hiking, am I likely to like running? There\u0026rsquo;s no arrow between those, and our graph implies there\u0026rsquo;s a relationship there, but we don\u0026rsquo;t know that we can safely infer such things. Plus, should we be conservative and say it\u0026rsquo;s 80% likely or should we be optimistic and say 90%, or average the two at 85%? We might even be more conservative and \u0026ldquo;walk the graph\u0026rdquo; and multiply .8 * .9 to get 72% likelihood (See picture 3 and 4). Plus, we know that this isn\u0026rsquo;t purely bidirectional, so someone who said they like \u0026ldquo;outdoor activities\u0026rdquo; might also like walking 70% of the time, but someone who likes walking may be 91% likely to like \u0026ldquo;outdoor activities\u0026rdquo;\u0026hellip;\nFor fun here\u0026rsquo;s some naive code. We can imagine inferring a few hops, reviewing dijkstra\u0026rsquo;s algo, etc.\ngraph = { (\u0026#34;running\u0026#34;, \u0026#34;walking\u0026#34;): 0.80, (\u0026#34;walking\u0026#34;, \u0026#34;running\u0026#34;): 0.80, (\u0026#34;hiking\u0026#34;, \u0026#34;walking\u0026#34;): 0.90, (\u0026#34;walking\u0026#34;, \u0026#34;hiking\u0026#34;): 0.90, } def score(a, b): if (a, b) in graph: return \u0026#34;direct\u0026#34;, f\u0026#34;{graph[(a, b)]:.4f}\u0026#34; shared = [x for x in [\u0026#34;running\u0026#34;, \u0026#34;hiking\u0026#34;, \u0026#34;walking\u0026#34;] if (a, x) in graph] if shared: return \u0026#34;one-hop\u0026#34;, f\u0026#34;{graph[(a, shared[0])] * graph[(shared[0], b)]:.4f}\u0026#34; return \u0026#34;no path\u0026#34;, 0 print(\u0026#34;running ‚Üí walking:\u0026#34;, score(\u0026#34;running\u0026#34;, \u0026#34;walking\u0026#34;)) print(\u0026#34;hiking ‚Üí walking:\u0026#34;, score(\u0026#34;hiking\u0026#34;, \u0026#34;walking\u0026#34;)) print(\u0026#34;running ‚Üí hiking:\u0026#34;, score(\u0026#34;running\u0026#34;, \u0026#34;hiking\u0026#34;)) running ‚Üí walking: (\u0026#39;direct\u0026#39;, \u0026#39;0.8000\u0026#39;) hiking ‚Üí walking: (\u0026#39;direct\u0026#39;, \u0026#39;0.9000\u0026#39;) running ‚Üí hiking: (\u0026#39;one-hop\u0026#39;, \u0026#39;0.7200\u0026#39;) Of course, it\u0026rsquo;s great if we can enumerate the nodes we need in the graph, but often there are too many terms we might not know. Consider that someone may say \u0026ldquo;I like running\u0026rdquo; or \u0026ldquo;I\u0026rsquo;m a runner\u0026rdquo; or \u0026ldquo;I go for runs\u0026rdquo; and simply searching for \u0026ldquo;running\u0026rdquo; as an interest will miss 2 terms out of three of these. Stemming and other tricks might help but they\u0026rsquo;re brittle. We always come across data we didn\u0026rsquo;t expect, e.g. we might read \u0026ldquo;I was a sprinter on the track team\u0026rdquo; or \u0026ldquo;I like sprinting\u0026rdquo; and miss that they have an interest in \u0026ldquo;running\u0026rdquo;.\nA newer way to look at this problem is to leverage ML. LLMs are great at exactly this kind of problem. For an application we may not want to use a full prompt and LLM API call, but luckily we can use embeddings and vectorDBs to help. Let\u0026rsquo;s quickly look at the word embeddings for walk, run, hike and see what we get in terms of cosine distances:\nimport chromadb from chromadb.utils import embedding_functions import numpy as np def main(): embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction( model_name=\u0026#34;average_word_embeddings_glove.6B.300d\u0026#34; # Word-optimized model ) data = [\u0026#34;walk\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;hike\u0026#34;] embeddings = embedding_fn(data) print(f\u0026#34;Embedding vector eg: sz={len(embeddings[0])}, looks like: {embeddings[0][:5]}...\u0026#34;) print(\u0026#34;Pairwise cosine similarities (0-1, 1.0=same):\u0026#34;) for i in range(len(data)): for j in range(i + 1, len(data)): sim = cosine_similarity(embeddings[i], embeddings[j]) print(f\u0026#34;* {data[i]:\u0026gt;6} \u0026lt;-\u0026gt; {data[j]:\u0026lt;6}: {sim:.4f}\u0026#34;) def cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2) return dot_product / norm_product if __name__ == \u0026#34;__main__\u0026#34;: main() Embedding vector eg: sz=300, looks like: [-0.014619 -0.17277 -0.11171 0.31864 -0.52504 ]... Pairwise cosine similarities (0-1, 1.0=same): * walk \u0026lt;-\u0026gt; run : 0.4750 * walk \u0026lt;-\u0026gt; hike : 0.3058 * run \u0026lt;-\u0026gt; hike : 0.2077 Interesting:. This model finds walk and run to be the most similar, walk and hike the next, and hike and run the least similar\nToday, we have embeddings, which give us a nice calculation of \u0026ldquo;conceptual distance\u0026rdquo;. What this means is we can get these \u0026ldquo;relationship\u0026rdquo; numbers \u0026ldquo;for free\u0026rdquo; without necessarily building this graph out. Plus vectorDBs are amazing at quickly giving us the top-K of similar items. Embedding models have done the work of figuring out, across huge corpuses of text, what these words and concepts mean, especially in relation to each other. And that\u0026rsquo;s probably good enough for a lot of use cases.\nThere\u0026rsquo;s a caveat here, which is to say that words without context can be dangerous. I worked in web search and we used to say \u0026ldquo;Fencing can be a sport, the stuff that borders your house, or what you do with stolen goods\u0026rdquo;. So, depending on your embedding model, you may want to calculate from full sentences such as \u0026ldquo;I like the activity walking\u0026rdquo; and \u0026ldquo;I like the activity running\u0026rdquo; as opposed to using the bare words.\nA quick change to the more popular Sentence model \u0026ldquo;all-MiniLLM-L6-v2\u0026rdquo;:\nembedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction( model_name=\u0026#34;all-MiniLM-L6-v2\u0026#34; # Sentence transformer model ) data = [\u0026#34;I like to walk\u0026#34;, \u0026#34;I like to run\u0026#34;, \u0026#34;I like to hike\u0026#34;] embeddings = embedding_fn(data[:3]) Embedding vector eg: sz=384, looks like: [-0.06094085 -0.05805941 0.03494389 0.09246249 0.0638584 ]... Pairwise cosine similarities (0-1, 1.0=same): * I like to walk \u0026lt;-\u0026gt; I like to run: 0.6254 * I like to walk \u0026lt;-\u0026gt; I like to hike: 0.7236 * I like to run \u0026lt;-\u0026gt; I like to hike: 0.5118 Interesting. This model has a different perspective, which could be the model or the context of interests and preferences (or both).\nThe most powerful and robust feature here is that we can handle almost any input without the need to pre-calculate our graph and weights. If we have an enumeration of categories of people\u0026rsquo;s favorite activities, and we see an event category that\u0026rsquo;s new to us, we can make an educated guess about how to map them, or use the embedding as input to our ranking. Meaning, if we put event descriptions into our vectorDB, then search for \u0026ldquo;I like to walk\u0026rdquo;, then a hiking event should score well.\nOf course, a combination of the two concepts would be needed in a real system since \u0026ldquo;interests\u0026rdquo; isn\u0026rsquo;t the same as \u0026ldquo;semantic meaning\u0026rdquo;. Here we controlled the sentence, but if you put in \u0026ldquo;I hate to walk\u0026rdquo; it turns out that sentence scores 0.7697 from \u0026ldquo;I like to walk\u0026rdquo; because these are close in latent space although one part of the vector is pointing in the opposite direction. If you can afford the latency of calling a foundational LLM with a prompt, it would handle such a case nicely.\nMy friend is still playing around with his code, but running through these ideas gave him some fun approaches to think about. Matching, recommendation systems, and ranking are always interesting to play with and there\u0026rsquo;s always more to explore.\nWhat\u0026rsquo;s your experience on these topics? Feel free to ask questions in the comments, or let me know what other topics you\u0026rsquo;d like to know about. I always monitor them for awhile after publishing.\nEnd Notes: # Production quality is all about the details, so you should always test and benchmark. A vectorDB search is fast, but if you need accuracy and control then the right approach may be building a graph and calculating these weights yourself (doing several queries to get answers). At the time of writing, Weaviate was the only vectorDB I found that directly supports graphDB features. We used a couple models, so beware that the embedding vectors between them are not compatible. e.g. never compare vectors from a word model against a sentence model. Thanks Richard King for the fencing examples ;-)\n","date":"12 December 2025","externalUrl":null,"permalink":"/posts/play-with-embeddings/","section":"Posts","summary":"From GraphDB Ontologies to Embeddings: Modeling Fuzzy Relationships # Keywords: GraphDB, VectorDB, Ontology, Embedding Vectors\nI was talking to a friend who uses a graphDB in their AI app and has run into an interesting problem. The quality is low because there are often many terms for the same thing, and many terms with multiple meanings depending on context. For example, if you‚Äôre looking for people to go on a walk with, it‚Äôs likely that people who like running or hiking are good candidates, but this may not be well represented in your graphDB. There‚Äôs a traditional way to solve this and more modern one using a vectorDB (weaviate, postgres with pgvector, etc.).\n","title":"Modeling The Fuzzy - from GraphDBs to Embeddings to VectorDBs","type":"posts"},{"content":"This is a personal blog for Charles Thayer (see also https://thayer.b2si.com). I practice and draft writing here. It\u0026rsquo;s in Hugo, with editing in Obsidian.\nAbout Me # I\u0026rsquo;m a software engineer, and sometimes founder. I like to tinkerer, build, think about thinking, and generally play with The Internet and related technologies.\nWelcome! üåû‚òïùõó\n","date":"22 November 2025","externalUrl":null,"permalink":"/posts/welcome/","section":"Posts","summary":"This is a personal blog for Charles Thayer (see also https://thayer.b2si.com). I practice and draft writing here. It‚Äôs in Hugo, with editing in Obsidian.\nAbout Me # I‚Äôm a software engineer, and sometimes founder. I like to tinkerer, build, think about thinking, and generally play with The Internet and related technologies.\nWelcome! üåû‚òïùõó\n","title":"Welcome","type":"posts"},{"content":" Types of Tests for AI Systems # At a simple level, evals are how we test AI systems, but the term \u0026ldquo;evals\u0026rdquo; covers a lot of ground. In reality there are lots of types of tests mainly grounded in an ML background. Let me explain. First there\u0026rsquo;s online vs offline, then there\u0026rsquo;s human vs LLM-as-a-judge, so that gives us 4 flavors off the bat. Plus there\u0026rsquo;s RAG which looks more like search bench-marking with it\u0026rsquo;s accuracy, precision, and recall. On top of these there are a variety of standard testing approaches and styles.\nDiagram: What you see here is Human vs Automated on the horizontal axis, which naturally separates between the manual human curate work, and the more involved automations where serious time and data need to be employed to get the best results. Similarly, we have Offline vs Online where the offline is happening on a developers bench (laptop or staging) and online is live in real-time with users or close to it.\nHuman Offline: here a developer is kicking off a benchmark or test run of some kind. In traditional ML you may be use test-evals the same way you would if testing a model you\u0026rsquo;ve been training. You have golden, ground truth, expected results \u0026ndash;both positive and negative feedback. If not binary, you may have many labels, like is this image \u0026ldquo;pizza\u0026rdquo; or \u0026ldquo;hotdog\u0026rdquo;, so this is when the work is generally supervised. Automated Offline: This is the next level, where you write a prompt to judge the output. You probably still do all the other things, but with the LLM-judge your trying to extend the human reviewers for leverage. This can also extend the inputs by generating synthetic random data and also judging that. Ideally you have occasional human supervision (like RLHF) but you\u0026rsquo;re probably finding cases that go back into your golden data set. Human Online: Here is where you have live feedback. When you get to this stage you have probably grown to the point where the scale has overwhelmed the humans. You\u0026rsquo;ve placed buttons on the UI for thumbs up / thumbs down feedback, maybe even provided a way for users to write about their reasoning, expectations, and intentions. This is the parallel to filing an AI \u0026ldquo;bug report\u0026rdquo; for review \u0026ndash;maybe even automated flagging. Automated Online: The is the automated equivalent. You may have a judge that looks are a sampling of conversations to detect unusual cases for further review. At this point, you have logging and traces (spans). Information, like high latency or cost automatically flag cases for review, without the user calling out poor performance. This covers a lot of ground, but I need to mention one more:\nConversations (Chats): these are hard to test and can follow many paths through an agent and tool calls. The best method I\u0026rsquo;m aware of is what I call Bot-Tests, and I\u0026rsquo;ve heard called LLM-as-a-user, persona tests, multi-turn conversation tests, etc. Here you set up an agent to converse as if it\u0026rsquo;s a user with enough background on how you\u0026rsquo;d like the conversation to evolve. A great example is in the medical context. Imagine you setup an agent to act as a patient with a specific illness, but to pretend it doesn\u0026rsquo;t know what the illness is, then gradually exhibit more and more symptoms.\nCode # A prompt A test - generic good bad llm-judge - a prompt, then a rubric llm-jury - a set of perspectives\ntest2\nReferences and Resources:\nLangFuse Evaluating Multi-Turn Conversations ","date":"9 June 2025","externalUrl":null,"permalink":"/posts/ai-testing-overview/","section":"Posts","summary":"Types of Tests for AI Systems # At a simple level, evals are how we test AI systems, but the term ‚Äúevals‚Äù covers a lot of ground. In reality there are lots of types of tests mainly grounded in an ML background. Let me explain. First there‚Äôs online vs offline, then there‚Äôs human vs LLM-as-a-judge, so that gives us 4 flavors off the bat. Plus there‚Äôs RAG which looks more like search bench-marking with it‚Äôs accuracy, precision, and recall. On top of these there are a variety of standard testing approaches and styles.\n","title":"AI Testing Overview","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]