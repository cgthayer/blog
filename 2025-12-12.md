TIL

I was talking to a friend who uses a graphDB in their AI app and has run into an interesting problem. The quality is low because there are often many terms for the same thing (and vice versa). For example, if you're looking for people to go on a walk with, it's likely that people who like running or hiking are good candidates, but this may not be well represented in your graphDB. There's a traditional way to solve this and more modern one using a vectorDB (weaviate, postgres with pgvector).

One approach is to create an ontology around this idea of activities and encode how related they are as a number. For example, if I like running maybe that means there's an 80% chance I'll like walking (even though I didn't say so), and if I like hiking perhaps there's a 90% chance I'll like walking. Plus I drew those arrows should probably be bidirectional, so if I like walking I'm also 90% likely to like hiking.

![graphdb-thoughts.excalidraw|800](graphdb-thoughts.excalidraw.md)


But wait, if I like hiking, am I likely to like running? Our graph implies there's a relationship there, but we don't know that we can safely infer such things. Plus, do we be conservative and say it's 80% likely or optimistic and say 90% or average the two at 85%. We might even be more conservative and "walk the graph" and multiple .8*.9 to get 72% likely (since we don't have direct evidence). Plus, we know that this isn't purely bidirectional, so someone who said they like "outdoor activities" might also like walking 70% of the time, but someone who likes walking is 91% likely to like "outdoor activities"...

But there's a newer way to look at this problem. We have vectorDBs and embeddings, which give us a nice calculation of "conceptual distance". What this means is, that we can get these kinda numbers "for free" without necessarily building this graph. Plus vectorDBs are amazing at quickly giving us the top-K of similar items. Embedding models have done the work of figuring out across huge corpuses of text what these words and concepts mean, especially in relation to each other. And that's probably good enough for a lot of use cases.

There's a caveat here, which is to say that words without context can be dangerous. I worked in web search and we used to say "Fencing can be a sport, the stuff the borders your house, or what you do with stolen goods". So, depending on your embedding model, you may want to calculate from "I like the activity walking" and "I like the activity running" as opposed to using the bare words.

End thought: it's all about the details, so you should always test. A vectorDB search is fast, but if you need accuracy and control then the right approach may be building a graph and calculating these weights yourself and doing several queries to get answers.

Thanks Dick King for the fencing examples

